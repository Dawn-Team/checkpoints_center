{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week Two 吴恩达《神经网络和深度学习》笔记\n",
    "## 2.11向量化Vectorization  \n",
    "经验法则:只要有其他可能，就不用使用显示for循环  \n",
    "GPU(也叫图像处理单元)、CPU都是SIMD指令(单指令流多数据流)  \n",
    "\n",
    "$z=w^Tx+b$  \n",
    "非向量化实现 python代码:  \n",
    " $z=0$  \n",
    " for i in range(n_x)  \n",
    " $z+=w[i]*x[i]$  \n",
    " $z+=b$  \n",
    " 向量化实现 python代码:  \n",
    " $z=np.dot(w,x)+b$    \n",
    "## 2.12向量化的更多例子More vectorization examples  \n",
    "np.log(v) 逐个元素计算log  \n",
    "np.abs(v) 计算绝对值  \n",
    "np.maximum(v,o) 计算所有元素中的最大值  \n",
    "v**2 v中每个元素的平方  \n",
    "1/v 每个元素求倒数  \n",
    "## 2.13向量化logistic回归Vectorizing Logistic Regression  \n",
    "$z=w^T+b$  \n",
    "$a=\\delta(z)$  \n",
    "$Z=np.dot(w.T,X)+b$  \n",
    "$A=\\delta(Z)$  \n",
    "正向传播一步迭代的向量化实现同时处理所有m个训练样本  \n",
    "## 2.14向量化logistic回归的梯度输出Vectorizing Logistic Regression's Gradient Computation  \n",
    "logistic回归的梯度下降一次迭代  \n",
    "$Z=w^TX+b=np.dot(w.T,X)+b$  \n",
    "$A=\\delta(Z)$  \n",
    "$dZ=A-Y$  \n",
    "$dw=\\frac{1}{m}XdZ^T$  \n",
    "$db==\\frac{1}{m}np.sum(dZ)$  \n",
    "$w:=w-\\alpha dw$  \n",
    "$b:=b-\\alpha db$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
