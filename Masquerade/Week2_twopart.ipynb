{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week Two 吴恩达《神经网络和深度学习》笔记\n",
    "## 2.4Gradient Descent梯度下降法\n",
    "1.review:  \n",
    "logistic回归算法: $\\hat{y} = sigmoid(w^{T}x +b) $ ,where $sigmoid(z)={\\frac{1}{1+e^{-z}}}$  \n",
    "成本函数: $J(w,b)={\\frac{1}{m}}\\sum_{i=1}^{m}L(\\hat{y}^{i},y^{i})$  \n",
    "\n",
    "2.α表示学习率,学习率可以控制每次迭代或者梯度下降法中的步长  \n",
    "对于J(w),repeat{$w:=w-\\alpha\\frac{dJ(w)}{dw}$}  \n",
    "对于J(w,b),repeat{$w:=w-\\alpha\\frac{\\delta J(w,b)}{\\delta w}$,$b:=b-\\alpha\\frac{\\delta J(w,b)}{\\delta b}$}  \n",
    "\n",
    "## 2.5Derivatives导数\n",
    "1.对于$f(a)=3a$，右移a一个不可度量的值、一个无限小的值,f(a)会增加一个非常小的值的3倍，而这个函数任何地方的斜率总是等于3  \n",
    "\n",
    "\n",
    "## 2.6More derivatives examples更多导数的例子\n",
    "1.为了便于理解,函数的导数就是函数的斜率，而函数的斜率在不同的点是不同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
