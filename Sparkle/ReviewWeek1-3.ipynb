{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Week1 Note of Deep Learning</center>\n",
    "### 1.Neural Network\n",
    "- **_1.1 concept:_  It is a powerful learning algorithm inspired by how the brain works.**  \n",
    "- **_1.2 Example1:_ single neural network**  \n",
    "Given data about the size of houses on the real estate market and you want to fit a function that will\n",
    "predict their price. It is a linear regression problem because the price as a function of size is a continuous\n",
    "output.<br>\n",
    "We know the prices can never be negative so we are creating a function called Rectified Linear Unit (ReLU)\n",
    "which starts at zero.\n",
    "- **_1.3 Example2:_ Multiple neural network**  （多元神经网络）\n",
    "The price of a house can be affected by other features such as size, number of bedrooms, zip code and\n",
    "wealth. The role of the neural network is to predicted the price and it will automatically generate the\n",
    "hidden units. We only need to give the inputs x(x1,x2,x3……) and the output y.\n",
    "\n",
    "### 2.Supervised learning for Neural Network （基于监督学习的神经网络）\n",
    "- **_2.1 CNN:_ Convolution Neural Network**  （卷积神经网络）\n",
    "CNN used often for image application\n",
    "- **_2.2 RNN:_ Recurrent Neural Network**  （循环神经网络）\n",
    "RNN used for one-dimensional sequence data such as translating English to Chinses or a temporal component such as text transcript. \n",
    "- **_2.2 Structured vs unstructured data:_**  \n",
    "Structured data refers to things that has a defined meaning such as price, age whereas unstructured\n",
    "data refers to thing like pixel, raw audio, text，image\n",
    "\n",
    "## <center>神经网络基础</center>\n",
    "### 1.二分分类算法：\n",
    "    例：isCat?\n",
    "    图片为RGB的三维向量，为了方便，需要将其转化为一维的特征向量vector\n",
    "    使用reshape方法\n",
    "    代码：v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1)\n",
    "\n",
    "    m_train 训练样本个数\n",
    "    m_test 测试集个数\n",
    "    将X,Y分为两个向量，以列排列。X为输入，Y为输出。\n",
    "    Python代码： X.shape = (nx,m) Y。shape = (1,m)\n",
    "    表示X矩阵的维度为nx行，m列。\n",
    "\n",
    "### 2.logistic回归\n",
    "    yhat 对y预测值  概率 (0,1)\n",
    "    线性回归 z = w^T*x+b \n",
    "    z = np.dot((w.T,X)+b) \n",
    "    由于我们需要yhat在（0，1）区间内\n",
    "    引入sigmoid函数：1.0/(1.0+np.exp(-z))\n",
    "    yhat = a = sigmoid(z)\n",
    "    \n",
    "    want yhat(i)约等于y(i)\n",
    "\n",
    "### 3.logistic回归损失函数（用于衡量预测输出值yhat和实际值y相差多少）（single traning sample）\n",
    "    L(yhat,y) = (1/2)*(yhat-y)^2 这种以平方差作为cost function在后续优化时会变成非凸函数，\n",
    "    会得到多个局部最优解，通过梯度下降法可能找不到全局最优解。\n",
    "    故将损失函数定义为：\n",
    "    L(yhat,y) = -(ylog(yhat)+(1-y)log(1-yhat))\n",
    "\n",
    "### 4.成本函数(用于衡量参数w和b的效果)（entire traning set）：凸函数\n",
    "    J(w,b) = 1/m(np.sum(L(yhat,y)))\n",
    "\n",
    "### 5.梯度下降法:\n",
    "    寻找使成本函数最小的参数w和b\n",
    "    朝最抖的下坡前进（迭代iterator）\n",
    "    get global optimum\n",
    "    dw表示f对w的偏导\n",
    "    learning rate (alpha)控制每一次迭代\n",
    "\n",
    "### 6.m个样本的梯度下降\n",
    "    J(w,b)\n",
    "    Logistic regression on m examples\n",
    "    When you are implementing deep learning algorithms.you'll find that having explicit for loops in your code makes your \n",
    "    algorithm run less efficiency.\n",
    "    there are set of techniques called vectorization techniques \n",
    "    使用向量化摆脱for循环。\n",
    "\n",
    "### 7.向量化：消除代码中显式for循环语句\n",
    "    训练大数据集时\n",
    "    什么是向量化，\n",
    "    在logistic回归中，z=w^Tx+b\n",
    "    for i in range(n-x);\n",
    "    z+ = w[i]*x[i]\n",
    "    z=np.dot(w,x)+b\n",
    "\n",
    "\n",
    "### 8.向量化的更多例子\n",
    "    1、u = Av(矩阵*向量)\n",
    "    vertorize:  u = np.dot(A,v)\n",
    "\n",
    "    2、指数运算(用到向量的每个元素)\n",
    "       import numpy as np\n",
    "       u=np.exp(v)  \n",
    "       u=np.log(v)  \n",
    "       u=np.abs(v)  \n",
    "       u=np.maximum(v,0)  \n",
    "       v**2： v中每个元素的平方  \n",
    "       1/v 求每个元素的的倒数  \n",
    "       dw = np.zeros((nx,1))  \n",
    "       dw+=x^(i)dz^(i)  \n",
    "       dw/=m  \n",
    "       \n",
    "## <center>浅层神经网络</center>\n",
    "### 1.神经网络表示：\n",
    "    神经网络基本的结构和符号可以从下面的图中看出，这里不再复述。\n",
    "    主要需要注意的一点，是层与层之间参数矩阵的规格大小：\n",
    "    输入层和隐藏层之间 \n",
    "    w[1]−>(4,3)：前面的4是隐层神经元的个数，后面的3是输入层神经元的个数；\n",
    "    b[1]−>(4,1)：和隐藏层的神经元个数相同；\n",
    "    隐藏层和输出层之间 \n",
    "    w[1]−>(1,4)：前面的1是输出层神经元的个数，后面的4是隐层神经元的个数；\n",
    "    b[1]−>(1,1)：和输出层的神经元个数相同；\n",
    "    由上面我们可以总结出，在神经网络中，我们以相邻两层为观测对象，前面一层作为输入，后面一层作为输出，两层之间的w参数矩阵大小为(nout,nin)，b参数矩阵大小为(nout,1)，这里是作为z=wX+b的线性关系来说明的，在神经网络中，w[i]=wT。\n",
    "    在logistic regression中，一般我们都会用(nin,nout)来表示参数大小，计算使用的公式为：z=wTX+b，要注意这两者的区别。\n",
    "    \n",
    "\n",
    "### 2.计算神经网络的输出\n",
    "    除输入层之外每层的计算输出可由下图总结出：\n",
    "    其中，每个结点都对应这两个部分的运算，z运算和a运算。  \n",
    "    在编程中，我们使用向量化去计算神经网络的输出：\n",
    "    在对应图中的神经网络结构，我们只用Python代码去实现右边的四个公式即可实现神经网络的输出计算。\n",
    "\n",
    "\n",
    "### 3.向量化实现\n",
    "    假定在m个训练样本的神经网络中，计算神经网络的输出，用向量化的方法去实现可以避免在程序中使用for循环，提高计算的速度。\n",
    "    下面是实现向量化的解释：\n",
    "    由图可以看出，在m个训练样本中，每次计算都是在重复相同的过程，均得到同样大小和结构的输出，所以利用向量化的思想将单个样本合并到一个矩阵中，其大小为(xn,m)，其中xn表示每个样本输入网络的神经元个数，也可以认为是单个样本的特征数，m表示训练样本的个数。\n",
    "    通过向量化，可以更加便捷快速地实现神经网络的计算。\n",
    "\n",
    "### 4.激活函数的选择:\n",
    "    激活函数的选择：\n",
    "\n",
    "    sigmoid函数和tanh函数比较：\n",
    "\n",
    "    隐藏层：tanh函数的表现要好于sigmoid函数，因为tanh取值范围为[−1,+1]，输出分布在0值的附近，均值为0，从隐藏层到输出层数据起到了归一化（均值为0）的效果。\n",
    "    输出层：对于二分类任务的输出取值为{0,1}，故一般会选择sigmoid函数。\n",
    "    然而sigmoid和tanh函数在当|z|很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使|z|尽可能的落在0值附近。\n",
    "\n",
    "    ReLU弥补了前两者的缺陷，当z>0时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。然而当z<0时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。\n",
    "\n",
    "    Leaky ReLU保证在z<0的时候，梯度仍然不为0。\n",
    "\n",
    "    在选择激活函数的时候，如果在不知道该选什么的时候就选择ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。\n",
    "\n",
    "\n",
    "### 5.神经网络的梯度下降法\n",
    "    以本节中的浅层神经网络为例，我们给出神经网络的梯度下降法的公式。\n",
    "\n",
    "    参数：W[1],b[1],W[2],b[2]；\n",
    "    输入层特征向量个数：nx=n[0]；\n",
    "    隐藏层神经元个数：n[1]；\n",
    "    输出层神经元个数：n[2]=1；\n",
    "    W[1]的维度为(n[1],n[0])，b[1]的维度为(n[1],1)；\n",
    "    W[2]的维度为(n[2],n[1])，b[2]的维度为(n[2],1)；\n",
    "    下面为该例子的神经网络反向梯度下降公式（左）和其代码向量化（右）：\n",
    "\n",
    "### 6.随机初始化\n",
    "    如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。\n",
    "\n",
    "    在初始化的时候，W参数要进行随机初始化，b则不存在对称性的问题它可以设置为0。\n",
    "\n",
    "    以2个输入，2个隐藏神经元为例：\n",
    "   ````\n",
    "   W = np.random.rand((2,2))* 0.01\n",
    "   b = np.zero((2,1))\n",
    "   ````\n",
    "    这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则Z=WX+b所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。\n",
    "\n",
    "    ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
