{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>神经网络基础</center>\n",
    "### 1.二分分类算法：\n",
    "    例：isCat?\n",
    "    图片为RGB的三维向量，为了方便，需要将其转化为一维的特征向量vector\n",
    "    使用reshape方法\n",
    "    代码：v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1)\n",
    "\n",
    "    m_train 训练样本个数\n",
    "    m_test 测试集个数\n",
    "    将X,Y分为两个向量，以列排列。\n",
    "\n",
    "### 2.logistic回归\n",
    "    yhat 对y预测值  概率 (0,1)\n",
    "    z = np.dot((w.T,X)+b)\n",
    "    yhat = a = sigmoid(z)\n",
    "    sigmoid函数  1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "### 3.logistic回归损失函数（用于衡量预测输出值yhat和实际值y相差多少）（single traning sample）\n",
    "    L(yhat,y) = (1/2)*(yhat-y)^2 这种以平方差作为cost function在后续优化时会变成非凸函数，\n",
    "    会得到多个局部最优解，通过梯度下降法可能找不到全局最优解。\n",
    "    故将损失函数定义为：\n",
    "    L(yhat,y) = -(ylog(yhat)+(1-y)log(1-yhat))\n",
    "\n",
    "### 4.成本函数(用于衡量参数w和b的效果)（entire traning set）：凸函数\n",
    "    J(w,b) = 1/m(np.sum(L(yhat,y)))\n",
    "\n",
    "### 5.梯度下降法:\n",
    "    寻找使成本函数最小的参数w和b\n",
    "    朝最抖的下坡前进（迭代iterator）\n",
    "    dw表示f对w的偏导\n",
    "\n",
    "    learning rate (alpha)控制每一次迭代\n",
    "\n",
    "\n",
    "### 6.m个样本的梯度下降\n",
    "    J(w,b)\n",
    "    Logistic regression on m examples\n",
    "    When you are implementing deep learning algorithms.you'll find that having explicit for loops in your code makes your \n",
    "    algorithm run less efficiency.\n",
    "    there are set of techniques called vectorization techniques \n",
    "    使用向量化摆脱for循环。\n",
    "\n",
    "### 7.向量化：消除代码中显式for循环语句\n",
    "    训练大数据集时\n",
    "    什么是向量化，\n",
    "    在logistic回归中，z=w^Tx+b\n",
    "    for i in range(n-x);\n",
    "    z+ = w[i]*x[i]\n",
    "    z=np.dot(w,x)+b\n",
    "\n",
    "\n",
    "### 8.向量化的更多例子\n",
    "    1、u = Av(矩阵*向量)\n",
    "    vertorize:  u = np.dot(A,v)\n",
    "\n",
    "    2、指数运算(用到向量的每个元素)\n",
    "       import numpy as np\n",
    "       u=np.exp(v)  \n",
    "       u=np.log(v)  \n",
    "       u=np.abs(v)  \n",
    "       u=np.maximum(v,0)  \n",
    "       v**2： v中每个元素的平方  \n",
    "       1/v 求每个元素的的倒数  \n",
    "       dw = np.zeros((nx,1))  \n",
    "       dw+=x^(i)dz^(i)  \n",
    "       dw/=m  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
