{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>浅深层神经网络</center>\n",
    "### 1.深层神经网络：\n",
    "    浅层神经网络\n",
    "    深层神经网络\n",
    "    注：计算神经网络层数时，不将输入层纳入计算，只计算上隐层和上输出层的数量\n",
    "    有些函数只有非常深层的神经网络能够学习\n",
    "    L：神经网络层数\n",
    "    n^[l]:节点数量\n",
    "    a^[l]:l层中的激活函数\n",
    "    w^[l]:在a^[l]中计算z^[l]的权重\n",
    "    \n",
    "\n",
    "### 2.深层网络中的前向传播(可以显式for循环)\n",
    "    z^[l] = w^[l]a^[l-1]+b^[l]\n",
    "    a^[l] = g^[l]( z^[l] )\n",
    "    向量化：\n",
    "    Z^[l] = W^[l]A^[l-1]+b^[l]\n",
    "    A^[l] = g^[l]( Z^[l] )\n",
    "\n",
    "\n",
    "### 3.核对矩阵维数\n",
    "    W^[l]:(n^[l],^[l-1])\n",
    "    b^[l]:(n^[l],1)\n",
    "    反向传播：\n",
    "    dw^[l]:(n^[l],^[l-1])\n",
    "    db^[l]:(n^[l],1)\n",
    "\n",
    "### 4.为什么使用深层表示:\n",
    "    较早的前几层能学习一些低层次的简单特征\n",
    "    等到后几层就能把简单特征结合起来去探测更加复杂东西\n",
    "\n",
    "\n",
    "### 5.搭建深层神经网络块\n",
    "\n",
    "    编程过程中需要将Z[l]，W[l]，b[l]缓存，在反向传播时可直接使用，节省时间 \n",
    "    \n",
    "\n",
    "### 6.前向传播和后向传播\n",
    "    forward propagation:\n",
    "    input:a^[l-1]\n",
    "    output:a^[l]，cache(z^[l])\n",
    "    backward propagation:\n",
    "    input:da^[l]\n",
    "    output:da^[l-1]，dW^[l]，db^[l]\n",
    "\n",
    "### 7.参数和超参数\n",
    "    超参数在某种程度上决定了参数 \n",
    "    超参数：学习率 alpha etc.  control parameters like w and b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
