{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键代码1：\n",
    "   layer_sizes(X, Y)函数\n",
    "     \n",
    "     n_x = X.shape[0] \n",
    "     n_h = 4\n",
    "     n_y = Y.shape[0]\n",
    "     \n",
    "  layer_sizes(X, Y)函数中确定特征数量和样本数量\n",
    "     \n",
    "### 关键代码2：\n",
    "   initialize_parameters(n_x, n_h, n_y)函数\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "   initialize_parameters(n_x, n_h, n_y)函数中，生成W的随机参数数组，但不能全部为0，不然即使输入不同样本，得到的激活函数都是一样的。b的参数数组可以为0\n",
    "   <br>np.random.randn(a,b)*00.1    标准正态分布中返回一个或多个样本值，乘以0.01是为了得到更小的z值，避免激活函数值位于激活函数的平缓部分，导致学习率降低</br>\n",
    "    <br>np.zeros((a,b)) 用于b的参数数组初始化0</br>\n",
    "    \n",
    "### 关键代码3：\n",
    "   forward_propagation(X, parameters)函数\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 =np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 =sigmoid(Z2)\n",
    "    \n",
    "forward_propagation(X, parameters)函数是正向传播函数\n",
    "     \n",
    " ### 关键代码4：\n",
    "    compute_cost(A2, Y, parameters)函数：\n",
    "     \n",
    "     logprobs = np.multiply(np.log(A2),Y)+(1-Y)*np.log(1-A2)\n",
    "     cost = - np.sum(logprobs)/m\n",
    "     \n",
    "     compute_cost(A2, Y, parameters)是计算损失的函数\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    " ### 关键代码5：\n",
    "    backward_propagation(parameters, cache, X, Y)函数:\n",
    "     \n",
    "     W1 = parameters[\"W1\"]\n",
    "     W2 = parameters[\"W2\"]\n",
    "     \n",
    "     A1 = cache[\"A1\"]\n",
    "     A2 = cache[\"A2\"]\n",
    "     \n",
    "     dZ2 = A2-Y\n",
    "     dW2 = 1/m*np.dot(dZ2,A1.T)\n",
    "     db2 = 1/m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "     dZ1 = np.dot(W2.T,dZ2)*(1 - np.power(A1, 2))\n",
    "     dW1 = 1/m*np.dot(dZ1,X.T)\n",
    "     db1 =1/m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "backward_propagation(parameters, cache, X, Y)函数是反向传播函数\n",
    "\n",
    " ### 关键代码6：\n",
    "    update_parameters(parameters, grads, learning_rate = 1.2)函数：\n",
    "     \n",
    "     W1 = parameters[\"W1\"]\n",
    "     b1 = parameters[\"b1\"]\n",
    "     W2 = parameters[\"W2\"]\n",
    "     b2 = parameters[\"b2\"]\n",
    "    \n",
    "\n",
    "     dW1 = grads[\"dW1\"]\n",
    "     db1 = grads[\"db1\"]\n",
    "     dW2 = grads[\"dW2\"]\n",
    "     db2 = grads[\"db2\"]\n",
    "\n",
    "\n",
    "     W1 = W1-learning_rate*dW1\n",
    "     b1 = b1-learning_rate*db1\n",
    "     W2 = W2-learning_rate*dW2\n",
    "     b2 = b2-learning_rate*db2\n",
    "     \n",
    "update_parameters(parameters, grads, learning_rate = 1.2)是参数更新函数，learning_rate是学习率，它决定了梯度下降的程度\n",
    "\n",
    "### 之后用函数创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
